{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 20\n",
    "\n",
    "def next_batch(set_choice, batch_sz):\n",
    "    if set_choice:\n",
    "        image, label = mnist.train.next_batch(batch_sz)\n",
    "    else:\n",
    "        image, label = mnist.test.next_batch(batch_sz)\n",
    "    digits = np.argmax(label, axis = 1)\n",
    "    false_index = np.where((digits ==0))[0].ravel()\n",
    "    true_index = np.where(digits ==1)[0].ravel()\n",
    "    true_image, false_image = image[true_index,:], image[false_index,:]\n",
    "    new_labels = np.hstack((np.full(np.shape(true_image)[0], 1),\n",
    "                     np.full(np.shape(false_image)[0], 0)))\n",
    "    indicies = [i for i in range(len(new_labels))]\n",
    "    np.random.shuffle(indicies)\n",
    "    to_return_images = np.vstack((true_image, false_image))[indicies,:]\n",
    "    new_labels = new_labels[indicies]\n",
    "    to_return_labels = np.zeros((len(new_labels), 2))\n",
    "    to_return_labels[np.arange(new_labels.size), new_labels] = 1\n",
    "    to_return_images = np.array([block_reduce(im.reshape(28,28), block_size=(3,3), \\\n",
    "                  func=np.mean).reshape(100) for im in to_return_images])\n",
    "    return (to_return_images, to_return_labels)\n",
    "\n",
    "training_images, training_labels = next_batch(True, mnist.train.num_examples)\n",
    "testing_images, testing_labels = next_batch(False, mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_examples(set_images, set_labels, batch_size):\n",
    "    indicies = np.random.choice(set_images.shape[0], batch_size)\n",
    "    return set_images[indicies,:], set_labels[indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(images[2,:].reshape(10,10))\n",
    "plt.show()\n",
    "\n",
    "def main_net_complex(x):\n",
    "    hidden_1 = tf.layers.dense(x, 500, activation = tf.nn.relu, use_bias = True)\n",
    "    hidden_2 = tf.layers.dense(hidden_1, 500, activation = tf.nn.relu, use_bias = True)\n",
    "    output_logits = tf.layers.dense(hidden_2, 2, activation = None, use_bias = True)\n",
    "    return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computational Graph\n",
    "delta,gamma = .5,1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x_ = tf.placeholder(tf.float32, name = \"InputImages\", shape= [None, 100])\n",
    "y_ = tf.placeholder(tf.float32, name = \"InputLabels\", shape = [None, 2])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([100,2], name = \"NetworkWeights\", stddev = 0.1))\n",
    "biases = tf.Variable(tf.random_normal([2], name = \"NetworkBiases\", stddev = 0.1))\n",
    "output_logits =  tf.matmul(x_, weights) + biases\n",
    "\n",
    "rl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = output_logits)\n",
    "\n",
    "chain_weights = tf.Variable(tf.random_normal([100,2], stddev = 0.1))\n",
    "chain_biases = tf.Variable(tf.random_normal([2], stddev = 0.1))\n",
    "\n",
    "expected_weights = tf.Variable(tf.zeros(dtype = tf.float32, shape = chain_weights.get_shape()))\n",
    "expected_biases = tf.Variable(tf.zeros(dtype = tf.float32, shape = chain_biases.get_shape()))\n",
    "                               \n",
    "c_output_logits =  tf.matmul(x_, chain_weights) + chain_biases\n",
    "cl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = c_output_logits)\n",
    "\n",
    "dcw = tf.gradients(cl, chain_weights)\n",
    "dcb = tf.gradients(cl, chain_biases)\n",
    "dc = tf.concat((tf.reshape(dcw, [-1]),tf.reshape(dcb, [-1])), axis = 0)\n",
    "c = tf.concat((tf.reshape(chain_weights, [-1]),tf.reshape(chain_biases, [-1])), axis = 0)\n",
    "\n",
    "#need to initialize this somewhere in computations below\n",
    "Xi_w = tf.Variable(tf.random_normal(weights.get_shape(), stddev = gamma))\n",
    "Xi_b = tf.Variable(tf.random_normal(biases.get_shape(), stddev = gamma))\n",
    "\n",
    "dpos = tf.constant(2+delta, dtype = tf.float32) \n",
    "dneg = tf.constant(2-delta, dtype = tf.float32)\n",
    "twod = tf.constant(2*delta, dtype = tf.float32)\n",
    "eightd = tf.constant(np.sqrt(8*delta), dtype = tf.float32)\n",
    "gam = tf.constant(gamma, dtype = tf.float32)\n",
    "\n",
    "proposal_w = ((dneg/dpos)*chain_weights)-((twod/dpos)*gam*dcw)+(eightd*Xi_w)\n",
    "proposal_b = ((dneg/dpos)*chain_biases)-((twod/dpos)*gam*dcb)+(eightd*Xi_b)\n",
    "\n",
    "p_output_logits =  tf.matmul(x_, tf.reshape(proposal_w,[100,2])) + proposal_b\n",
    "pl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = p_output_logits)\n",
    "\n",
    "dpw = tf.gradients(pl, proposal_w)\n",
    "dpb = tf.gradients(pl, proposal_b)\n",
    "dp = tf.concat((tf.reshape(dpw, [-1]),tf.reshape(dpb, [-1])), axis = 0)        \n",
    "p = tf.concat((tf.reshape(proposal_w, [-1]),tf.reshape(proposal_b, [-1])), axis = 0)\n",
    "\n",
    "half = tf.constant(1/2, dtype = tf.float32)\n",
    "dfour = tf.constant(delta/4, dtype = tf.float32)\n",
    "\n",
    "#These Values must have shape [1]. I need to sort that out soon...\n",
    "\n",
    "p_u_v = p + half * tf.tensordot((c-p), dp, axes = 1) \\\n",
    "           + dfour * tf.tensordot((c+p), dp, axes = 1) \\\n",
    "           + dfour *(eightd*dp)**2\n",
    "\n",
    "p_v_u = c + half * tf.tensordot((p-c), dc, axes = 1) \\\n",
    "           + dfour * tf.tensordot((c+p), dc, axes = 1) \\\n",
    "           + dfour *(eightd*dc)**2\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = .001).minimize(rl)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_pCN(delta, cycles, burn_in, batches, dim):\n",
    "        '''\n",
    "        Method Used to train NN via modified pCN algorithm\n",
    "        '''\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            \n",
    "            sess.run(tf.initialize_variables([chain_weights,\n",
    "                                              chain_biases,\n",
    "                                              expected_weights,\n",
    "                                              expected_biases]))\n",
    "            acceptance_count = 0\n",
    "            counter = 0\n",
    "            \n",
    "            if dim[0] == 100:\n",
    "                batch_images, batch_labels = query_examples(training_images, training_labels, batch_sz) \n",
    "            else:\n",
    "                batch_images, batch_labels = mnist.train.next_batch(batch_sz)\n",
    "\n",
    "            for i in range(cycles):\n",
    "                       \n",
    "                puv, pvu, pw, pb, cw, cb = sess.run([p_u_v,\n",
    "                                                     p_v_u,\n",
    "                                                     proposal_w,\n",
    "                                                     proposal_b,\n",
    "                                                     chain_weights,\n",
    "                                                     chain_biases],\n",
    "                                   feed_dict = {x_ : batch_images,\n",
    "                                                y_ : batch_labels})\n",
    "\n",
    "                alpha = np.min((np.exp(np.linalg.norm(puv - pvu)), 1))\n",
    "                \n",
    "                if np.random.uniform(0,1) < alpha:\n",
    "                    up_w = tf.assign(chain_weights, pw.reshape((dim[0],dim[1])))\n",
    "                    up_b = tf.assign(chain_biases, pb.reshape(dim[1]))\n",
    "                    sess.run([up_w, up_b])\n",
    "                    if (i > burn_in):\n",
    "                        acceptance_count += 1\n",
    "                        counter += 1\n",
    "                        ew_up = tf.assign(expected_weights, tf.add(expected_weights, chain_weights))\n",
    "                        eb_up = tf.assign(expected_biases, tf.add(expected_biases, chain_biases))\n",
    "                        sess.run([up_w, up_b, ew_up, eb_up])\n",
    "                else:\n",
    "                    if (i > burn_in):\n",
    "                        counter += 1\n",
    "                        ew_up = tf.assign(expected_weights, tf.add(expected_weights, chain_weights))\n",
    "                        eb_up = tf.assign(expected_biases, tf.add(expected_biases, chain_biases))\n",
    "                        sess.run([ew_up, eb_up])\n",
    "            \n",
    "            \n",
    "            ew, eb = sess.run([expected_weights, expected_biases])\n",
    "            new_weights = tf.assign(weights, (ew/counter))\n",
    "            new_biases = tf.assign(biases, (eb/counter))\n",
    "            sess.run([new_weights, new_biases])\n",
    "            print('Final Acceptance Ratio: ' + str((acceptance_count/counter)))\n",
    "                \n",
    "        print ('\\n' + 'Langevin pCN training complete')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD\n",
      "training progress: 0.0\n",
      "training progress: 0.1\n",
      "training progress: 0.2\n",
      "training progress: 0.3\n",
      "training progress: 0.4\n",
      "training progress: 0.5\n",
      "training progress: 0.6\n",
      "training progress: 0.7\n",
      "training progress: 0.8\n",
      "training progress: 0.9\n",
      "\n",
      "SGD Training Complete, Test ACC of: 0.5475177\n",
      "\n",
      "Begining pCN Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.276761 -2.459141]\n",
      "Final Acceptance Ratio: 1.0\n",
      "[-4.400471  -2.3345618]\n",
      "Final Acceptance Ratio: 1.0\n",
      "[-4.5968513 -2.13831  ]\n",
      "Final Acceptance Ratio: 1.0\n",
      "[-4.0832453 -2.6529267]\n",
      "Final Acceptance Ratio: 1.0\n",
      "[-4.067588  -2.6697557]\n",
      "Final Acceptance Ratio: 1.0\n",
      "\n",
      "Langevin pCN training complete\n",
      "test accuracy is:0.855792\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "batch_sz = 20\n",
    "\n",
    "print(\"Training with SGD\")\n",
    "num_batch = 10\n",
    "for i in range(num_batch): \n",
    "    train_i, train_l = query_examples(training_images, training_labels, batch_sz)\n",
    "    acc,_ = sess.run([accuracy, train_step], feed_dict = {x_: train_i, y_: train_l})\n",
    "    print(\"training progress: \" + str(float(i)/num_batch))\n",
    "    \n",
    "acc = sess.run(accuracy, feed_dict = {x_: testing_images, y_: testing_labels})\n",
    "print(\"\\n\" + \"SGD Training Complete, Test ACC of: \" + str(acc) + \"\\n\")\n",
    "\n",
    "print(\"Begining pCN Training:\")\n",
    "langevin_pCN(delta, 20, 5, 5)\n",
    "\n",
    "print(\"test accuracy is:\" + str(sess.run(accuracy, feed_dict = {x_ : testing_images,y_ : testing_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computational Graph\n",
    "delta,gamma = .5, .5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x_ = tf.placeholder(tf.float32, name = \"InputImages\", shape= [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, name = \"InputLabels\", shape = [None, 10])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([784,10], name = \"NetworkWeights\", stddev = 0.1))\n",
    "biases = tf.Variable(tf.random_normal([10], name = \"NetworkBiases\", stddev = 0.1))\n",
    "output_logits =  tf.matmul(x_, weights) + biases\n",
    "\n",
    "rl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = output_logits)\n",
    "\n",
    "chain_weights = tf.Variable(tf.random_normal([784,10], stddev = 0.1))\n",
    "chain_biases = tf.Variable(tf.random_normal([10], stddev = 0.1))\n",
    "\n",
    "expected_weights = tf.Variable(tf.zeros(dtype = tf.float32, shape = chain_weights.get_shape()))\n",
    "expected_biases = tf.Variable(tf.zeros(dtype = tf.float32, shape = chain_biases.get_shape()))\n",
    "                               \n",
    "\n",
    "c_output_logits =  tf.matmul(x_, chain_weights) + chain_biases\n",
    "cl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = c_output_logits)\n",
    "\n",
    "dcw = tf.gradients(cl, chain_weights)\n",
    "dcb = tf.gradients(cl, chain_biases)\n",
    "dc = tf.concat((tf.reshape(dcw, [-1]),tf.reshape(dcb, [-1])), axis = 0)\n",
    "c = tf.concat((tf.reshape(chain_weights, [-1]),tf.reshape(chain_biases, [-1])), axis = 0)\n",
    "\n",
    "#need to initialize this somewhere in computations below\n",
    "Xi_w = tf.Variable(tf.random_normal(weights.get_shape(), stddev = gamma))\n",
    "Xi_b = tf.Variable(tf.random_normal(biases.get_shape(), stddev = gamma))\n",
    "\n",
    "dpos = tf.constant(2+delta, dtype = tf.float32) \n",
    "dneg = tf.constant(2-delta, dtype = tf.float32)\n",
    "twod = tf.constant(2*delta, dtype = tf.float32)\n",
    "eightd = tf.constant(np.sqrt(8*delta), dtype = tf.float32)\n",
    "gam = tf.constant(gamma, dtype = tf.float32)\n",
    "\n",
    "proposal_w = ((dneg/dpos)*chain_weights)-((twod/dpos)*gam*dcw)+(eightd*Xi_w)\n",
    "proposal_b = ((dneg/dpos)*chain_biases)-((twod/dpos)*gam*dcb)+(eightd*Xi_b)\n",
    "\n",
    "p_output_logits =  tf.matmul(x_, tf.reshape(proposal_w,[784,10])) + proposal_b\n",
    "pl = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = p_output_logits)\n",
    "\n",
    "dpw = tf.gradients(pl, proposal_w)\n",
    "dpb = tf.gradients(pl, proposal_b)\n",
    "dp = tf.concat((tf.reshape(dpw, [-1]),tf.reshape(dpb, [-1])), axis = 0)        \n",
    "p = tf.concat((tf.reshape(proposal_w, [-1]),tf.reshape(proposal_b, [-1])), axis = 0)\n",
    "\n",
    "half = tf.constant(1/2, dtype = tf.float32)\n",
    "dfour = tf.constant(delta/4, dtype = tf.float32)\n",
    "\n",
    "p_u_v = p + half * tf.tensordot((c-p), dp, axes = 1) \\\n",
    "           + dfour * tf.tensordot((c+p), dp, axes = 1) \\\n",
    "           + dfour *(eightd*dp)**2\n",
    "\n",
    "p_v_u = c + half * tf.tensordot((p-c), dc, axes = 1) \\\n",
    "           + dfour * tf.tensordot((c+p), dc, axes = 1) \\\n",
    "           + dfour *(eightd*dc)**2\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = .001).minimize(rl)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD\n",
      "training progress: 0.0\n",
      "training progress: 0.01\n",
      "training progress: 0.02\n",
      "training progress: 0.03\n",
      "training progress: 0.04\n",
      "training progress: 0.05\n",
      "training progress: 0.06\n",
      "training progress: 0.07\n",
      "training progress: 0.08\n",
      "training progress: 0.09\n",
      "training progress: 0.1\n",
      "training progress: 0.11\n",
      "training progress: 0.12\n",
      "training progress: 0.13\n",
      "training progress: 0.14\n",
      "training progress: 0.15\n",
      "training progress: 0.16\n",
      "training progress: 0.17\n",
      "training progress: 0.18\n",
      "training progress: 0.19\n",
      "training progress: 0.2\n",
      "training progress: 0.21\n",
      "training progress: 0.22\n",
      "training progress: 0.23\n",
      "training progress: 0.24\n",
      "training progress: 0.25\n",
      "training progress: 0.26\n",
      "training progress: 0.27\n",
      "training progress: 0.28\n",
      "training progress: 0.29\n",
      "training progress: 0.3\n",
      "training progress: 0.31\n",
      "training progress: 0.32\n",
      "training progress: 0.33\n",
      "training progress: 0.34\n",
      "training progress: 0.35\n",
      "training progress: 0.36\n",
      "training progress: 0.37\n",
      "training progress: 0.38\n",
      "training progress: 0.39\n",
      "training progress: 0.4\n",
      "training progress: 0.41\n",
      "training progress: 0.42\n",
      "training progress: 0.43\n",
      "training progress: 0.44\n",
      "training progress: 0.45\n",
      "training progress: 0.46\n",
      "training progress: 0.47\n",
      "training progress: 0.48\n",
      "training progress: 0.49\n",
      "training progress: 0.5\n",
      "training progress: 0.51\n",
      "training progress: 0.52\n",
      "training progress: 0.53\n",
      "training progress: 0.54\n",
      "training progress: 0.55\n",
      "training progress: 0.56\n",
      "training progress: 0.57\n",
      "training progress: 0.58\n",
      "training progress: 0.59\n",
      "training progress: 0.6\n",
      "training progress: 0.61\n",
      "training progress: 0.62\n",
      "training progress: 0.63\n",
      "training progress: 0.64\n",
      "training progress: 0.65\n",
      "training progress: 0.66\n",
      "training progress: 0.67\n",
      "training progress: 0.68\n",
      "training progress: 0.69\n",
      "training progress: 0.7\n",
      "training progress: 0.71\n",
      "training progress: 0.72\n",
      "training progress: 0.73\n",
      "training progress: 0.74\n",
      "training progress: 0.75\n",
      "training progress: 0.76\n",
      "training progress: 0.77\n",
      "training progress: 0.78\n",
      "training progress: 0.79\n",
      "training progress: 0.8\n",
      "training progress: 0.81\n",
      "training progress: 0.82\n",
      "training progress: 0.83\n",
      "training progress: 0.84\n",
      "training progress: 0.85\n",
      "training progress: 0.86\n",
      "training progress: 0.87\n",
      "training progress: 0.88\n",
      "training progress: 0.89\n",
      "training progress: 0.9\n",
      "training progress: 0.91\n",
      "training progress: 0.92\n",
      "training progress: 0.93\n",
      "training progress: 0.94\n",
      "training progress: 0.95\n",
      "training progress: 0.96\n",
      "training progress: 0.97\n",
      "training progress: 0.98\n",
      "training progress: 0.99\n",
      "\n",
      "SGD Training Complete, Test ACC of: 0.093\n",
      "\n",
      "Begining pCN Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Acceptance Ratio: 1.0\n",
      "Final Acceptance Ratio: 1.0\n",
      "Final Acceptance Ratio: 1.0\n",
      "Final Acceptance Ratio: 1.0\n",
      "Final Acceptance Ratio: 1.0\n",
      "\n",
      "Langevin pCN training complete\n",
      "test accuracy is:0.1621\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "batch_sz = 20\n",
    "\n",
    "print(\"Training with SGD\")\n",
    "num_batch = 100\n",
    "for i in range(num_batch): \n",
    "    train_i, train_l = mnist.train.next_batch(batch_sz)\n",
    "    #acc,_ = sess.run([accuracy, train_step], feed_dict = {x_: train_i, y_: train_l})\n",
    "    print(\"training progress: \" + str(float(i)/num_batch))\n",
    "    \n",
    "t_images, t_labels = mnist.test.next_batch(mnist.test.num_examples)\n",
    "acc = sess.run(accuracy, feed_dict = {x_: t_images, y_: t_labels})\n",
    "print(\"\\n\" + \"SGD Training Complete, Test ACC of: \" + str(acc) + \"\\n\")\n",
    "\n",
    "print(\"Begining pCN Training:\")\n",
    "langevin_pCN(delta, 100, 5, 5, [784, 10])\n",
    "\n",
    "print(\"test accuracy is:\" + str(sess.run(accuracy, feed_dict = {x_ : t_images,y_ : t_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEG",
   "language": "python",
   "name": "seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
